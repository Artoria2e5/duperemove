{
  "name": "Duperemove",
  "tagline": "Tools for deduping file systems",
  "body": "This README is for the development branch of duperemove. If you're looking\r\nfor a stable version which is continually updated with fixes, please see\r\n[v0.10 branch](https://github.com/markfasheh/duperemove/tree/v0.10-branch).\r\n\r\n# Duperemove\r\n\r\nDuperemove is a simple tool for finding duplicated extents and\r\nsubmitting them for deduplication. When given a list of files it will\r\nhash their contents on a block by block basis and compare those hashes\r\nto each other, finding and categorizing extents that match each\r\nother. When given the -d option, duperemove will submit those\r\nextents for deduplication using the btrfs-extent-same ioctl.\r\n\r\nDuperemove can store the hashes it computes in a hashfile. If\r\ngiven an existing hashfile, duperemove will only compute hashes\r\nfor those files which have changed since the last run.  Thus you can run\r\nduperemove repeatedly on your data as it changes, without having to\r\nre-checksum unchanged data.\r\n\r\nDuperemove can also take input from the fdupes program.\r\n\r\n\r\n## Readonly / Non-deduplicating Mode\r\n\r\nDuperemove has two major modes of operation one of which is a subset\r\nof the other.\r\n\r\nWhen run without -d (the default) duperemove will print out one or\r\nmore tables of matching extents it has determined would be ideal\r\ncandidates for deduplication. As a result, readonly mode is useful for\r\nseeing what duperemove might do when run with '-d'. The output could\r\nalso be used by some other software to submit the extents for\r\ndeduplication at a later time.\r\n\r\nIt is important to note that this mode will not print out *all*\r\ninstances of matching extents, just those it would consider for\r\ndeduplication.\r\n\r\nGenerally, duperemove does not concern itself with the underlying\r\nrepresentation of the extents it processes. Some of them could be\r\ncompressed, undergoing I/O, or even have already been deduplicated. In\r\ndedupe mode, the kernel handles those details and therefore we try not\r\nto replicate that work.\r\n\r\n\r\n## Deduping Mode\r\n\r\nThis functions similarly to readonly mode with the exception that the\r\nduplicated extents found in our \"read, hash, and compare\" step will\r\nactually be submitted for deduplication. An estimate of the total data\r\ndeduplicated will be printed after the operation is complete. This\r\nestimate is calculated by comparing the total amount of shared bytes\r\nin each file before and after the dedupe.\r\n\r\n\r\nSee the duperemove man page for further details about running duperemove.\r\n\r\n\r\n# Requirements\r\n\r\nThe latest stable code can be found in [v0.10-branch](https://github.com/markfasheh/duperemove/tree/v0.10-branch).\r\n\r\nKernel: Duperemove needs a kernel version equal to or greater than 3.13\r\n\r\nLibraries: Duperemove uses glib2 and sqlite3.\r\n\r\n\r\n# FAQ\r\n\r\nPlease see the FAQ file [provided in the duperemove\r\nsource](https://github.com/markfasheh/duperemove/blob/master/FAQ.md)\r\n\r\n# Simple Usage Example\r\n\r\nPlease see the duperemove man page for more interesting usage examples.\r\n\r\nDuperemove takes a list of files and directories to scan for\r\ndedupe. If a directory is specified, all regular files within it will\r\nbe scanned. Duperemove can also be told to recursively scan\r\ndirectories with the '-r' switch. If '-h' is provided, duperemove will\r\nprint numbers in powers of 1024 (e.g., \"128K\").\r\n\r\nAssume this abitrary layout for the following examples.\r\n\r\n    .\r\n    ├── dir1\r\n    │   ├── file3\r\n    │   ├── file4\r\n    │   └── subdir1\r\n    │       └── file5\r\n    ├── file1\r\n    └── file2\r\n\r\nThis will dedupe files 'file1' and 'file2':\r\n\r\n    duperemove -dh file1 file2\r\n\r\nThis does the same but adds any files in dir1 (file3 and file4):\r\n\r\n    duperemove -dh file1 file2 dir1\r\n\r\nThis will dedupe exactly the same as above but will recursively walk\r\ndir1, thus adding file5.\r\n\r\n    duperemove -dhr file1 file2 dir1/\r\n\r\n\r\nAn actual run, output will differ according to duperemove version.\r\n\r\n    duperemove -dhr file1 file2 dir1\r\n    Using 128K blocks\r\n    Using hash: SHA256\r\n    Using 2 threads for file hashing phase\r\n    csum: file1     [1/5]\r\n    csum: file2     [2/5]\r\n    csum: dir1/file3       [3/5]\r\n    csum: dir1/subdir1/file5       [4/5]\r\n    csum: dir1/file4       [5/5]\r\n    Hashed 80 blocks, resulting in 17 unique hashes. Calculating duplicate\r\n    extents - this may take some time.\r\n    [########################################]\r\n    Search completed with no errors.\r\n    Simple read and compare of file data found 2 instances of extents that might\r\n    benefit from deduplication.\r\n    Start           Length          Filename (2 extents)\r\n    0.0     2.0M    \"file2\"\r\n    0.0     2.0M    \"dir1//file4\"\r\n    Start           Length          Filename (3 extents)\r\n    0.0     2.0M    \"file1\"\r\n    0.0     2.0M    \"dir1//file3\"\r\n    0.0     2.0M    \"dir1//subdir1/file5\"\r\n    Dedupe 1 extents with target: (0.0, 2.0M), \"file2\"\r\n    Dedupe 2 extents with target: (0.0, 2.0M), \"file1\"\r\n    Kernel processed data (excludes target files): 6.0M\r\n    Comparison of extent info shows a net change in shared extents of: 10.0M\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}